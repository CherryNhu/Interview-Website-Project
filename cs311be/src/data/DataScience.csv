STT,Câu hỏi,Trả lời
1,Xác suất biên (Marginal Probability) là gì?,"Xác suất biên là xác suất xảy ra của một sự kiện cụ thể, mà không quan tâm đến các sự kiện khác. Ví dụ, nếu bạn đang xem xét xác suất mưa vào ngày mai, bạn chỉ quan tâm đến khả năng mưa, không phải các điều kiện thời tiết khác như gió hoặc nhiệt độ."
2,Các tiên đề xác suất (Probability Axioms) là gì?,"Các tiên đề xác suất là các quy tắc cơ bản giúp chúng ta hiểu cách hoạt động của xác suất. Có ba tiên đề chính: Tiên đề phi âm (Non-Negativity Axiom): Xác suất không thể âm. Khả năng xảy ra của một sự kiện luôn ≥ 0, không bao giờ nhỏ hơn. Tiên đề chuẩn hóa (Normalization Axiom): Nếu một sự kiện chắc chắn xảy ra (như mặt trời mọc vào ngày mai), xác suất của nó là 1. Vậy, 1 nghĩa là ""chắc chắn xảy ra"". Tiên đề cộng (Additivity Axiom): Nếu hai sự kiện không thể xảy ra cùng lúc (như tung xúc xắc ra số 3 hoặc 4), xác suất xảy ra một trong hai là tổng của các xác suất riêng lẻ."
3,Sự khác biệt giữa các sự kiện phụ thuộc và độc lập trong xác suất là gì?,"Các sự kiện độc lập (Independent Events): Hai sự kiện độc lập nếu một sự kiện không thay đổi khả năng xảy ra của sự kiện kia. Ví dụ, tung đồng xu hai lần – lần tung đầu không ảnh hưởng đến lần thứ hai. Vậy, xác suất cả hai xảy ra là tích của các xác suất riêng lẻ. Các sự kiện phụ thuộc (Dependent Events): Hai sự kiện phụ thuộc nếu một sự kiện ảnh hưởng đến khả năng xảy ra của sự kiện kia. Ví dụ, rút một lá bài từ bộ bài và không đặt lại (không thay thế), xác suất rút lá thứ hai phụ thuộc vào lá đầu tiên. Xác suất thay đổi vì một lá đã được rút ra."
4,Xác suất có điều kiện (Conditional Probability) là gì?,"Xác suất có điều kiện là xác suất xảy ra của một sự kiện khi biết rằng một sự kiện khác đã xảy ra. Toán học, nó được định nghĩa là xác suất sự kiện A xảy ra, khi sự kiện B đã xảy ra, ký hiệu là P(A│B). Công thức là: P(A│B) = P(A ∩ B) / P(B), trong đó P(A│B) là xác suất có điều kiện của A khi B, P(A ∩ B) là xác suất chung của cả A và B xảy ra cùng lúc, P(B) là xác suất của B xảy ra."
5,Định lý Bayes là gì và khi nào chúng ta sử dụng nó trong Data Science?,"Định lý Bayes giúp tính xác suất xảy ra của một sự kiện dựa trên kiến thức trước hoặc bằng chứng. Nó giống như cập nhật dự đoán của chúng ta khi học được thông tin mới. Công thức: P(A│B) = [P(B│A) * P(A)] / P(B), trong đó P(A│B) là xác suất A xảy ra khi B đã xảy ra, P(B│A) là xác suất B xảy ra khi A xảy ra, P(A) là xác suất A xảy ra bất kể B, P(B) là xác suất B xảy ra bất kể A."
6,Định nghĩa phương sai (Variance) và phương sai có điều kiện (Conditional Variance).,"Phương sai là cách đo lường mức độ phân tán hoặc khác biệt của các số trong tập dữ liệu so với trung bình. Nếu các số gần trung bình, phương sai thấp; nếu phân tán xa, phương sai cao. Giống như đo lường sự khác biệt điểm thi của mọi người so với điểm trung bình lớp. Phương sai có điều kiện tương tự, nhưng xem xét sự thay đổi của một biến khi biết điều gì đó khác về nó. Ví dụ, đo lường sự biến thiên chiều cao dựa trên độ tuổi; phương sai có điều kiện cho biết sự biến thiên chiều cao trong một nhóm tuổi cụ thể, sử dụng kiến thức về tuổi để tập trung vào biến thiên trong nhóm đó."
7,"Giải thích các khái niệm Trung bình (Mean), Trung vị (Median), Mốt (Mode) và Độ lệch chuẩn (Standard Deviation).","Trung bình (Mean): Là giá trị trung tâm của tập số, tính bằng tổng các số chia cho số lượng. Nó đại diện cho dữ liệu tổng thể. Trung vị (Median): Là số ở giữa khi sắp xếp dữ liệu từ nhỏ đến lớn. Nếu số lượng chẵn, lấy trung bình hai số giữa. Trung vị không bị ảnh hưởng bởi giá trị cực đoan, tốt hơn cho trung tâm khi có ngoại lai. Mốt (Mode): Là số xuất hiện thường xuyên nhất. Có thể có một, nhiều hoặc không có mốt. Độ lệch chuẩn (Standard Deviation): Đo lường mức độ phân tán của số so với trung bình. Nếu gần trung bình, độ lệch chuẩn nhỏ; nếu phân tán, lớn. Nó cho thấy sự biến thiên hoặc ""phân tán"" trong dữ liệu."
8,Phân phối chuẩn (Normal Distribution) và Phân phối chuẩn chuẩn hóa (Standard Normal Distribution) là gì?,"Phân phối chuẩn: Là đường cong hình chuông, hầu hết dữ liệu gần trung bình (mean), càng xa mean càng ít dữ liệu. Thường thấy trong tự nhiên như chiều cao hoặc điểm thi. Phân phối chuẩn chuẩn hóa: Là loại phân phối chuẩn đặc biệt với mean = 0 và độ lệch chuẩn = 1. Giúp so sánh giữa các tập dữ liệu dễ dàng hơn vì dữ liệu được chuẩn hóa."
9,Sự khác biệt giữa tương quan (correlation) và nhân quả (causation) là gì?,"Tương quan nghĩa là hai thứ liên quan hoặc xảy ra cùng lúc, nhưng một không nhất thiết gây ra cái kia. Ví dụ, ăn kem nhiều hơn vào mùa hè và bơi lội nhiều hơn, có tương quan nhưng ăn kem không gây bơi lội. Nhân quả nghĩa là một thứ trực tiếp gây ra cái kia. Ví dụ, học nhiều hơn dẫn đến điểm cao hơn. Để chứng minh nhân quả, cần bằng chứng từ thí nghiệm."
10,"Phân phối đều (Uniform), Bernoulli và Nhị thức (Binomial) là gì và chúng khác nhau như thế nào?","Phân phối đều (Uniform Distribution): Mọi kết quả có xác suất bằng nhau. Ví dụ, tung xúc xắc công bằng, mỗi số 1-6 có xác suất như nhau, đồ thị là đường thẳng phẳng. Phân phối Bernoulli: Dùng cho hai kết quả có thể, thành công hoặc thất bại. Ví dụ, tung đồng xu: heads (thành công) hoặc tails (thất bại). Phân phối nhị thức (Binomial): Áp dụng cho số lượng thử độc lập cố định, mỗi thử có hai kết quả. Tính xác suất số thành công cụ thể, ví dụ tung đồng xu 5 lần, xác suất đúng 3 heads."
11,Giải thích phân phối mũ (Exponential Distribution) và nơi nó thường được sử dụng.,"Phân phối mũ mô tả thời gian giữa các sự kiện ngẫu nhiên xảy ra với tốc độ không đổi. Ví dụ, thời gian chờ khách tiếp theo đến cửa hàng hoặc thời gian bóng đèn cháy trước khi hỏng."
12,Mô tả phân phối Poisson và đặc điểm của nó.,"Phân phối Poisson cho biết tần suất xảy ra sự kiện trong khoảng thời gian hoặc không gian nhất định. Dùng khi sự kiện xảy ra với tốc độ ổn định, như số xe qua trạm thu phí trong một giờ. Các điểm chính: Đếm số sự kiện xảy ra. Sự kiện xảy ra với tốc độ không đổi. Mỗi sự kiện độc lập, một sự kiện không ảnh hưởng đến cái khác."
13,Giải thích phân phối t (t-distribution) và mối quan hệ của nó với phân phối chuẩn (normal distribution).,"Phân phối t tương tự phân phối chuẩn, nhưng dùng khi dữ liệu ít và không biết độ phân tán dân số chính xác. Nó rộng hơn và phân tán hơn phân phối chuẩn, nhưng khi dữ liệu nhiều hơn, nó giống phân phối chuẩn hơn."
14,Mô tả phân phối chi-bình phương (chi-squared distribution).,Phân phối chi-bình phương dùng để kiểm tra dữ liệu khớp với mô hình nào đó hoặc xem hai thứ có liên quan. Thường dùng trong kiểm định như kiểm tra xúc xắc công bằng hoặc liên kết giữa tuổi và sở thích bỏ phiếu.
15,"Sự khác biệt giữa kiểm định z (z-test), kiểm định F (F-test) và kiểm định t (t-test) là gì?","Kiểm định z (Z-test): Dùng để so sánh trung bình mẫu với trung bình dân số đã biết và biết độ lệch chuẩn dân số. Thường dùng với mẫu lớn hoặc thông tin tốt về dân số. Kiểm định t (T-test): Tương tự z-test, nhưng dùng khi không biết độ lệch chuẩn dân số. Thường dùng với mẫu nhỏ. Kiểm định F (F-test): Dùng để so sánh độ phân tán (variance) giữa hai hoặc nhiều nhóm. Ví dụ, xem hai phương pháp dạy có dẫn đến kết quả khác nhau ở học sinh không."
16,Định lý giới hạn trung tâm (Central Limit Theorem) là gì và tại sao nó quan trọng trong thống kê?,"Định lý giới hạn trung tâm (CLT) nói rằng nếu lấy nhiều mẫu từ dân số, dù dân số trông thế nào, trung bình của các mẫu sẽ giống phân phối chuẩn (hình chuông) khi kích thước mẫu lớn hơn. Điều này quan trọng vì cho phép dùng quy tắc phân phối chuẩn để dự đoán, ngay cả nếu dân số không chuẩn."
17,"Mô tả quy trình kiểm định giả thuyết (hypothesis testing), bao gồm giả thuyết null và alternative.","Kiểm định giả thuyết giúp quyết định xem một tuyên bố về dân số có đúng dựa trên dữ liệu mẫu. Giả thuyết null (H0): Giả định ""không hiệu quả"", nghĩa là không có gì xảy ra hoặc thay đổi. Giả thuyết alternative (H1): Ngược lại, gợi ý có thay đổi hoặc hiệu quả. Chúng ta thu thập dữ liệu và kiểm tra xem nó hỗ trợ H1 không. Nếu bằng chứng đủ, bác bỏ H0."
18,Bạn tính khoảng tin cậy (confidence interval) như thế nào và nó đại diện cho gì?,"Khoảng tin cậy cho phạm vi giá trị mà chúng ta tin rằng giá trị dân số thực nằm trong đó, dựa trên dữ liệu mẫu. Để tính: Thu thập dữ liệu mẫu, tính trung bình mẫu và sai số biên (mức biến thiên có thể). Khoảng tin cậy là phạm vi quanh trung bình mà giá trị dân số nên nằm, với mức tin cậy nhất định (như 95%)."
19,Giá trị p (p-value) trong thống kê là gì?,"Giá trị p cho biết khả năng chúng ta nhận được dữ liệu như vậy nếu giả thuyết null đúng. Giá trị p nhỏ (dưới 0.05) nghĩa là dữ liệu không chắc dưới H0, vậy có thể bác bỏ H0. Giá trị p lớn nghĩa là dữ liệu phù hợp với H0, không bác bỏ."
20,Giải thích lỗi loại I và loại II trong kiểm định giả thuyết.,"Lỗi loại I (False Positive): Sai lầm bác bỏ giả thuyết null đúng, nghĩ có thay đổi khi không có. Lỗi loại II (False Negative): Không bác bỏ giả thuyết null sai, bỏ lỡ hiệu quả thực."
21,"Mức ý nghĩa (significance level, alpha) trong kiểm định giả thuyết là gì?","Mức ý nghĩa (alpha) là ngưỡng để quyết định bác bỏ giả thuyết null. Nó cho thấy mức rủi ro chấp nhận lỗi loại I (bác bỏ null sai). Thường alpha = 0.05, nghĩa là 5% cơ hội lỗi loại I."
22,Bạn tính hệ số tương quan (correlation coefficient) giữa hai biến như thế nào?,"Hệ số tương quan đo lường mức độ liên quan mạnh giữa hai biến. Để tính: Thu thập dữ liệu cho cả hai biến. Tìm trung bình mỗi biến. Tính hiệp phương sai (covariance) – cách chúng di chuyển cùng nhau. Chia cho tích độ lệch chuẩn để chuẩn hóa. Kết quả từ -1 đến 1: 1 là quan hệ dương hoàn hảo, -1 âm hoàn hảo, 0 không liên quan."
23,Hiệp phương sai (covariance) là gì và nó liên quan đến tương quan như thế nào?,"Hiệp phương sai cho thấy hai biến thay đổi cùng nhau. Nếu cả hai tăng cùng, dương; một tăng một giảm, âm. Nhưng phụ thuộc quy mô biến, khó so sánh. Tương quan chuẩn hóa hiệp phương sai bằng độ lệch chuẩn, dễ diễn giải từ -1 đến 1 cho sức mạnh và hướng quan hệ."
24,Giải thích cách thực hiện kiểm định giả thuyết để so sánh trung bình hai dân số.,"Khi so sánh trung bình hai dân số: 1. Thiết lập giả thuyết: H0: Hai trung bình bằng nhau. H1: Hai trung bình khác. Thu thập dữ liệu từ cả hai. 2. Tính thống kê kiểm định (thường t-test hoặc z-test). 3. So sánh kết quả xem sự khác biệt có ý nghĩa thống kê. 4. Nếu khác biệt đủ lớn, bác bỏ H0."
25,Giải thích phân phối đa biến (multivariate distribution) trong data science.,"Phân phối đa biến liên quan nhiều biến và giúp mô hình hóa tình huống quan tâm đến mối quan hệ giữa các biến. Ví dụ, dự đoán giá nhà dựa trên kích thước, vị trí và tuổi nhà. Nó xem cách các đặc trưng khác nhau hoạt động cùng nhau và ảnh hưởng đến kết quả."
26,Mô tả khái niệm hàm mật độ xác suất có điều kiện (Conditional Probability Density Function - PDF).,"Hàm mật độ xác suất có điều kiện (PDF) mô tả xác suất xảy ra sự kiện khi biết sự kiện khác đã xảy ra. Ví dụ, xác suất một người bị bệnh khi có triệu chứng nhất định. Nó giúp hiểu cách một sự kiện ảnh hưởng đến xác suất sự kiện khác."
27,Hàm phân phối tích lũy (Cumulative Distribution Function - CDF) là gì và nó liên quan đến PDF như thế nào?,"Hàm phân phối tích lũy (CDF) cho xác suất biến ngẫu nhiên liên tục nằm dưới giá trị nhất định. PDF mô tả xác suất trong khoảng, CDF là tích phân của PDF. PDF là đạo hàm của CDF, chúng liên quan qua tích phân và vi phân."
28,ANOVA là gì? Các cách khác nhau để thực hiện kiểm định ANOVA là gì?,"ANOVA (Analysis of Variance) là phương pháp thống kê kiểm tra biến thiên trong tập dữ liệu và xác định sự khác biệt ý nghĩa giữa trung bình nhóm. Dùng để so sánh trung bình nhiều nhóm hoặc phương pháp. Các loại: One-Way ANOVA (một yếu tố), Two-Way ANOVA (hai yếu tố). Khi thực hiện, tính F-statistic và so sánh với giá trị tới hạn hoặc p-value."
29,Sự khác biệt giữa dân số (population) và mẫu (sample) trong thống kê là gì?,"Dân số: Toàn bộ nhóm muốn nghiên cứu. Ví dụ, chiều cao trung bình tất cả học sinh trường, dân số là mọi học sinh. Mẫu: Phần nhỏ của dân số. Vì không thể nghiên cứu tất cả, chọn vài người đại diện. Ví dụ, đo 100 học sinh để ước lượng chiều cao trung bình toàn trường."
30,Các loại học máy (machine learning) khác nhau là gì?,"Học có giám sát (Supervised Learning): Máy được cho dữ liệu có nhãn đúng. Ví dụ, ảnh chó mèo có nhãn, máy học để nhận diện ảnh mới. Học không giám sát (Unsupervised Learning): Dữ liệu không nhãn, máy tự tìm mẫu hoặc nhóm. Ví dụ, nhóm ảnh chó mèo mà không nói trước."
31,Hồi quy tuyến tính (linear regression) là gì và các giả định khác nhau của thuật toán hồi quy tuyến tính là gì?,"Hồi quy tuyến tính là loại học có giám sát tính quan hệ tuyến tính giữa biến dự đoán và phản hồi. Dựa trên phương trình: ŷ = β1 x + β0, với ŷ là biến phụ thuộc, β1 là độ dốc, β0 là giao điểm, x là biến độc lập. Các giả định: Quan hệ tuyến tính (thay đổi x thay đổi y tuyến tính). Phân phối chuẩn (dữ liệu đối xứng quanh mean). Độc lập (đặc trưng không tương quan). Đồng phương sai (variance bằng nhau cho tất cả biến dự đoán)."
32,Hồi quy logistic là kỹ thuật phân loại và tại sao tên nó là hồi quy chứ không phải phân loại logistic?,"Mặc dù hồi quy logistic dùng cho phân loại, nó vẫn giữ cấu trúc hồi quy bên dưới. Ý tưởng chính là mô hình hóa xác suất xảy ra sự kiện (lớp 1 trong phân loại nhị phân) bằng tổ hợp tuyến tính đặc trưng, rồi áp dụng hàm logistic (sigmoid) để biến thành xác suất từ 0 đến 1. Sự biến đổi này làm nó phù hợp cho phân loại, nhưng giữ đặc tính toán học của mô hình hồi quy."
33,Hàm logistic (sigmoid function) trong hồi quy logistic là gì?,"Hàm logistic hoặc sigmoid dùng để dự đoán xác suất trong hồi quy logistic. Nó lấy số thực bất kỳ và ánh xạ thành giá trị từ 0 đến 1, tốt cho kết quả nhị phân như ""có"" hoặc ""không"". Công thức: f(x) = 1 / (1 + e^(-x)). Nếu đầu ra gần 1, dự đoán một lớp; gần 0, lớp kia."
34,Overfitting là gì và làm thế nào để khắc phục?,"Overfitting là khi mô hình khớp quá sát dữ liệu huấn luyện, không tổng quát hóa tốt với dữ liệu mới. Xảy ra khi học nhiễu từ dữ liệu huấn luyện. Để tránh: Chọn đặc trưng (feature selection): Chỉ dùng đặc trưng cần thiết. Xác thực chéo (Cross Validation): Chia dữ liệu thành các lô nhỏ để điều chỉnh mô hình. Chính quy hóa (Regularization): Thêm phạt vào hàm mất mát để giảm overfitting. Mô hình ensemble: Kết hợp kết quả từ nhiều mô hình."
35,Máy vector hỗ trợ (Support Vector Machine - SVM) là gì và các thành phần chính của nó là gì?,"SVM là thuật toán học có giám sát dùng cho hồi quy và phân loại. Mục tiêu tìm siêu phẳng phân tách dữ liệu thành lớp. Dữ liệu mới được phân loại dựa trên siêu phẳng. Hiệu quả với không gian chiều cao và dữ liệu phi tuyến, nhưng dễ overfitting nếu đặc trưng > mẫu. Thành phần chính: Hàm kernel: Ánh xạ dữ liệu vào không gian chiều cao. Siêu phẳng: Ranh giới quyết định phân biệt lớp. Biên (Margin): Khoảng cách giữa vector hỗ trợ và siêu phẳng. C: Tham số chính quy hóa cân bằng biên tối đa và giảm phân loại sai."
36,Giải thích thuật toán k-nearest neighbors (KNN).,"Thuật toán KNN là học có giám sát đơn giản cho phân loại và hồi quy. Nó dự đoán bằng cách nhớ dữ liệu thay vì xây mô hình. Gọi là ""học lười"" hoặc dựa trên bộ nhớ. KNN dựa trên nguyên tắc điểm dữ liệu tương tự thuộc cùng lớp hoặc có giá trị mục tiêu tương tự. Trong huấn luyện, lưu toàn bộ dữ liệu. Để dự đoán, tính khoảng cách đến tất cả điểm huấn luyện (thường Euclidean hoặc Manhattan), chọn k điểm gần nhất và lấy đa số phiếu (phân loại) hoặc trung bình (hồi quy). Chọn k phù hợp quan trọng: k nhỏ nhiễu, k lớn làm mịn biên quyết định. Cần chuẩn hóa đặc trưng."
37,Thuật toán Naïve Bayes là gì và các giả định khác nhau của Naïve Bayes là gì?,"Naïve Bayes là thuật toán phân loại xác suất dựa trên định lý Bayes với giả định ""naïve"" rằng đặc trưng độc lập trong mỗi lớp. Thường dùng cho phân loại nhị phân và đa lớp, đặc biệt khi cần đơn giản, nhanh. Giả định: Độc lập đặc trưng – đặc trưng có điều kiện độc lập, sự hiện diện một không ảnh hưởng cái khác. Bình đẳng – đặc trưng bằng nhau về tầm quan trọng. Chuẩn – phân phối đặc trưng là chuẩn, dữ liệu phân bố đều quanh mean."
38,Cây quyết định (Decision Trees) là gì và chúng hoạt động như thế nào?,"Cây quyết định là thuật toán học máy phổ biến cho phân loại và hồi quy. Chúng tạo cấu trúc cây quyết định dựa trên đặc trưng đầu vào để dự đoán. Gồm nút và cạnh: Bắt đầu từ nút gốc, phân nhánh thành nút nội bộ đại diện đặc trưng với quy tắc quyết định chia dữ liệu thành tập con. Cạnh kết nối nút chỉ quyết định hoặc kết quả. Nút lá đại diện dự đoán cuối. Mục tiêu tăng tính đồng nhất dữ liệu, đo bằng MSE (hồi quy) hoặc Gini impurity (phân loại). Có thể xử lý nhiều loại thuộc tính và nắm bắt quan hệ phức tạp, nhưng dễ overfitting nếu sâu. Giảm bằng cắt tỉa hoặc giới hạn độ sâu."
39,Giải thích khái niệm Entropy và Information Gain trong cây quyết định.,"Entropy là đo lường độ hỗn loạn hoặc không chắc chắn dữ liệu. Nếu tất cả thuộc cùng lớp, entropy thấp; nếu phân tán nhiều lớp, cao. Công thức: H(S) = -∑(pi log2(pi)), pi là xác suất mỗi lớp. Information Gain cho biết giảm không chắc chắn sau khi chia bằng đặc trưng. Cao hơn nghĩa là đặc trưng giúp tổ chức dữ liệu tốt hơn. Là sự khác biệt entropy trước và sau chia. Công thức: IG = H(S) - ∑("
40,Sự khác biệt giữa mô hình Bagging và Boosting là gì?,"Bagging và Boosting là kỹ thuật ensemble cải thiện độ chính xác bằng cách kết hợp nhiều mô hình, nhưng khác cách hoạt động. Bagging: Huấn luyện nhiều mô hình độc lập trên phần ngẫu nhiên dữ liệu, kết hợp dự đoán bằng trung bình hoặc phiếu. Ví dụ: Random Forest. Boosting: Huấn luyện mô hình tuần tự, mỗi cái sửa lỗi trước, kết hợp với trọng số cao hơn cho mô hình tốt. Ví dụ: AdaBoost, Gradient Boosting."
41,Mô tả Random Forests và lợi thế của chúng so với Cây quyết định đơn lẻ.,"Random Forests là kỹ thuật ensemble kết hợp nhiều cây quyết định để cải thiện độ chính xác và giảm overfitting. Lợi thế so với cây đơn: Tổng quát hóa tốt hơn: Cây đơn dễ overfitting, Random Forests trung bình dự đoán từ nhiều cây, tổng quát tốt hơn trên dữ liệu mới. Xử lý dữ liệu chiều cao tốt: Chọn tập con đặc trưng ngẫu nhiên cho mỗi cây, cải thiện khi có nhiều đặc trưng không liên quan. Bền vững với ngoại lai: Kết hợp dự đoán từ nhiều cây xử lý trường hợp cực đoan tốt hơn."
42,K-Means là gì và nó hoạt động như thế nào?,"K-Means là thuật toán học không giám sát dùng cho phân cụm, nhóm điểm dữ liệu tương tự. Nhắm chia dữ liệu thành K cụm, mỗi cụm đại diện nhóm điểm gần nhau theo đo lường tương tự. Cách hoạt động: Chọn K (số cụm). Gán mỗi điểm đến cụm gần nhất với tâm (centroid). Tính lại tâm dựa trên gán hiện tại. Lặp lại đến khi ổn định."
43,Ma trận nhầm lẫn (Confusion Matrix) là gì? Giải thích với ví dụ.,"Ma trận nhầm lẫn là bảng đánh giá hiệu suất mô hình phân loại bằng cách so sánh dự đoán với nhãn thực. Cung cấp thông tin cho độ chính xác, precision, recall. Ví dụ ma trận ung thư: Hàng: Thực ung thư / Không ung thư. Cột: Dự đoán ung thư / Không. TP: Đúng dương (dự đoán ung thư đúng). FP: Sai dương. FN: Sai âm. TN: Đúng âm."
44,Báo cáo phân loại (Classification Report) là gì và giải thích các tham số dùng để diễn giải kết quả nhiệm vụ phân loại với ví dụ.,"Báo cáo phân loại tóm tắt hiệu suất mô hình phân loại, cung cấp metrics đánh giá chất lượng dự đoán. Tham số: Precision: Tỷ lệ TP / (TP + FP), độ chính xác dự đoán dương. Recall: TP / (TP + FN), khả năng xác định tất cả dương thực. Accuracy: (TP + TN) / Tổng, độ đúng tổng thể. F1-Score: 2 * (Precision * Recall) / (Precision + Recall), cân bằng precision và recall, hữu ích với dữ liệu mất cân bằng."
45,Chính quy hóa (Regularization) trong học máy là gì? Nêu sự khác biệt giữa L1 và L2 regularization.,"Chính quy hóa là kỹ thuật ngăn mô hình quá phức tạp và overfitting bằng cách thêm phạt vào hàm chi phí, giữ mô hình đơn giản hơn, hiệu suất tốt hơn trên dữ liệu mới. L1 (Lasso): Thêm giá trị tuyệt đối hệ số vào chi phí, khuyến khích thưa thớt, đặt một số trọng số = 0, hữu ích chọn đặc trưng. L2 (Ridge): Thêm bình phương hệ số, giảm kích thước tất cả hệ số nhưng không = 0, giữ tất cả đặc trưng nhưng ít ảnh hưởng hơn."
46,Giải thích khái niệm đánh đổi bias-variance trong học máy.,"Đánh đổi bias-variance là khái niệm chính trong học máy, cân bằng hai nguồn lỗi: bias và variance để chọn mô hình tốt. Bias: Sai lệch do mô hình không dự đoán đúng giá trị thực (underfitting). Variance: Biến thiên hiệu suất khi huấn luyện trên tập con khác (overfitting). Mục tiêu: Mô hình bias thấp (fit dữ liệu tốt) và variance thấp (không thay đổi nhiều với dữ liệu khác). Bias cao: Underfitting. Variance cao: Overfitting. Đánh đổi tìm cân bằng tốt nhất."
47,Naïve Bayes xử lý đặc trưng phân loại và liên tục như thế nào?,"Naïve Bayes tính xác suất lớp dựa trên đặc trưng, giả định độc lập. Với đặc trưng phân loại (màu sắc, có/không): Xem tần suất mỗi loại trong lớp để tính xác suất. Với đặc trưng liên tục (chiều cao, cân nặng): Giả định phân phối chuẩn, dùng mean và độ lệch chuẩn của lớp để tính xác suất. Chọn lớp có xác suất cao nhất cho dữ liệu mới."
48,Laplace smoothing (add-one smoothing) là gì và tại sao dùng trong Naïve Bayes?,"Trong Naïve Bayes, xác suất có điều kiện P(sự kiện"
49,Tập dữ liệu mất cân bằng (imbalanced datasets) là gì và cách xử lý chúng?,"Tập dữ liệu mất cân bằng là khi phân bố nhãn lớp lệch, một lớp có nhiều instance hơn. Mô hình thiên vị lớp đa số, hiệu suất kém lớp thiểu số. Cách xử lý: Resampling: Up-sampling (tăng lớp thiểu bằng lấy mẫu không thay thế hoặc tổng hợp). Down-sampling (giảm lớp đa số). Ensemble: Bagging (Random Forest giảm tác động mất cân bằng). Boosting (AdaBoost, XGBoost ưu tiên lớp thiểu bị sai)."
50,Ngoại lai (outliers) trong tập dữ liệu là gì và cách phát hiện và loại bỏ chúng?,"Ngoại lai là điểm dữ liệu khác biệt đáng kể so với các điểm khác, thường ở cực phân phối. Phát hiện: Kiểm tra hình ảnh (scatter plot, box plot). Thống kê (khoảng cách từ mean/median). Z-score (cao quá là ngoại lai). Loại bỏ: Thủ công, biến đổi (log, square root), thay thế (mean, median, mode)."
51,Lời nguyền chiều (curse of dimensionality) là gì và cách khắc phục?,"Khi dữ liệu có chiều cao (nhiều đặc trưng), gặp vấn đề: Chi phí tính toán cao, dữ liệu thưa (điểm xa nhau), khó hình dung và overfitting. Khắc phục: Chọn đặc trưng (loại bỏ không cần). Kỹ thuật đặc trưng (kết hợp đặc trưng giảm số lượng). Giảm chiều (PCA, t-SNE). Chính quy hóa (L1, L2 quyết định tác động đặc trưng)."
52,Mô tả gradient descent và vai trò của nó trong tối ưu hóa mô hình học máy.,Gradient descent là thuật toán tối ưu hóa giảm hàm chi phí bằng cách điều chỉnh tham số lặp lại. Vai trò: Giảm chi phí (tìm tham số tốt nhất). Hội tụ (lặp đến tiêu chí dừng). Tổng quát hóa (mô hình tốt trên dữ liệu mới).
53,Thuật toán random forest xử lý chọn đặc trưng như thế nào?,"Random Forest xử lý chọn đặc trưng: Khi tạo cây, gán tập con đặc trưng ngẫu nhiên cho mỗi cây (feature bagging), tạo đa dạng. Sau huấn luyện, gán điểm quan trọng dựa trên giảm lỗi mô hình. Xếp hạng đặc trưng theo điểm, cao hơn nghĩa là ảnh hưởng lớn hơn đến dự đoán."
54,Kỹ thuật đặc trưng (Feature Engineering) là gì? Giải thích các phương pháp kỹ thuật đặc trưng khác nhau.,"Kỹ thuật đặc trưng là tiền xử lý dữ liệu để phân tích tốt hơn, bao gồm chọn, biến đổi, xóa đặc trưng phù hợp vấn đề. Lợi ích: Cải thiện hiệu suất mô hình, diễn giải dữ liệu, giảm chi phí tính toán, phát hiện mẫu ẩn. Phương pháp: PCA (giảm chiều bằng thành phần chính). Mã hóa (chuyển dữ liệu thành số có ý nghĩa: One-Hot cho nominal, Label cho ordinal). Biến đổi đặc trưng (tạo cột mới bằng kết hợp hoặc sửa đổi)."
55,Chúng ta xử lý giá trị văn bản phân loại trong học máy như thế nào?,"Dữ liệu văn bản phân loại (nam/nữ, hạng 1/2/3) chia thành: Nominal (không thứ tự, ví dụ nam/nữ): One-Hot encoding thành chuỗi nhị phân. Ordinal (có thứ tự): Label encoding, chuyển số dựa trên thứ tự."
56,DBSCAN là gì và chúng ta sử dụng nó như thế nào?,"DBSCAN (Density-Based Spatial Clustering of Applications with Noise) là thuật toán phân cụm dựa mật độ, nhóm điểm gần nhau ở vùng mật độ cao, đánh dấu điểm thấp mật độ là ngoại lai. Cách: Tính khoảng cách mỗi điểm đến tất cả. Xác định vùng mật độ bằng lõi điểm trong ngưỡng eps. Hình thành cụm bằng cách nhóm điểm đạt mật độ từ nhau."
57,Thuật toán EM (Expectation-Maximization) hoạt động như thế nào trong phân cụm?,"EM là cách tiếp cận xác suất cho phân cụm với mô hình hỗn hợp, khi không biết gán cụm thực và có không chắc chắn. Cách: Chỉ định K cụm. Bước E: Tính khả năng điểm thuộc mỗi cụm. Bước M: Cập nhật tham số mô hình. Lặp đến hội tụ (so log-likelihood hoặc tham số)."
58,Giải thích khái niệm điểm silhouette trong đánh giá phân cụm.,"Điểm silhouette đánh giá chất lượng cụm. Tính: (a) Khoảng cách trung bình điểm đến điểm cùng cụm. (b) Khoảng cách trung bình đến cụm lân cận gần nhất. Hệ số S = (b - a) / max(a, b). S gần 1: Điểm tốt trong cụm. Gần 0: Gần biên. Âm: Gần cụm khác hơn cụm mình."
59,Mối quan hệ giữa giá trị riêng (eigenvalues) và vector riêng (eigenvectors) trong PCA là gì?,"Trong PCA, eigenvalues và eigenvectors quan trọng cho biến đổi dữ liệu. Eigenvectors: Hướng trục mà dữ liệu biến thiên nhiều nhất. Eigenvalues: Độ lớn biến thiên dọc eigenvector đó. Quan hệ: A V = λ V (A ma trận đặc trưng, V eigenvector, λ eigenvalue). Eigenvalue lớn hơn nắm bắt nhiều biến thiên hơn. Tổng eigenvalues = tổng biến thiên, tỷ lệ giải thích = eigenvalue / tổng."
60,Kỹ thuật xác thực chéo (Cross Validation) trong học máy là gì?,"Xác thực chéo là kỹ thuật lấy mẫu lại đánh giá hiệu suất mô hình, ước lượng hiệu suất trên dữ liệu chưa thấy, tránh overfitting. Các loại: K-Fold: Chia K tập con, huấn luyện/test K lần. Stratified K-Fold: Giữ tỷ lệ lớp gốc (xử lý mất cân bằng). Shuffle-Split: Xáo trộn và chia ngẫu nhiên."
61,ROC và AUC là gì? Giải thích ý nghĩa của chúng trong phân loại nhị phân.,"ROC (Receiver Operating Characteristic) là đồ thị TPR (True Positive Rate = Recall = TP/(TP+FN)) vs FPR (False Positive Rate = FP/(TN+FP)) ở các ngưỡng khác nhau. AUC (Area Under Curve) là diện tích dưới đường cong, từ 0-1: 0.5 đoán ngẫu nhiên, 1 phân loại hoàn hảo. AUC lượng hóa hiệu suất tổng thể mô hình phân loại nhị phân."
62,"Mô tả Batch Gradient Descent, Stochastic Gradient Descent và Mini-Batch Gradient Descent.","Batch GD: Dùng toàn bộ dữ liệu huấn luyện tính gradient mỗi lần lặp, cập nhật tham số sau. Chính xác nhưng chậm với dữ liệu lớn. Stochastic GD: Dùng một ví dụ ngẫu nhiên mỗi lần, cập nhật nhanh nhưng biến thiên cao, có thể chậm hội tụ. Mini-Batch GD: Dùng tập con nhỏ (mini-batch) mỗi lần, cân bằng tốc độ và ổn định, tận dụng GPU."
63,Giải thích Apriori - Khai thác quy tắc liên kết (Association Rule Mining).,"Khai thác quy tắc liên kết tìm mẫu hoặc quan hệ giữa item trong dữ liệu lớn, như sản phẩm thường mua cùng. Apriori dùng thuộc tính Apriori: Nếu nhóm item thường mua cùng, tất cả nhóm nhỏ hơn cũng thường. Bắt đầu tìm item thường, rồi tổ hợp lớn hơn. Ví dụ, nếu thường mua bánh mì và bơ, gợi ý mua bơ khi mua bánh mì."
64,Làm thế nào để ngăn Gradient Descent bị kẹt ở cực tiểu cục bộ?,"Để tránh kẹt cực tiểu cục bộ: Điểm bắt đầu tốt (Xavier/He initialization). Tối ưu hóa thông minh (Adam, RMSProp điều chỉnh tốc độ học). Ngẫu nhiên với mini-batch (giúp nhảy ra). Tăng độ phức tạp mô hình (thêm lớp/neuron). Điều chỉnh siêu tham số (tìm kiếm ngẫu nhiên/grid search)."
65,Giải thích các thuật toán Gradient Boosting trong học máy.,"Gradient Boosting như XGBoost, CatBoost dùng cho hồi quy/phân loại, kết hợp người học yếu thành mạnh. Bước: Khởi tạo với cây yếu. Tính phần dư (sai lệch dự đoán-thực). Thêm người học mới nắm bắt phần dư. Cập nhật mô hình bằng phần học mới (kiểm soát bởi tốc độ học). Lặp, mỗi lần sửa lỗi trước."
66,"SQL là gì, và nó viết tắt của gì?","SQL (Structured Query Language) là ngôn ngữ lập trình chuyên dụng quản lý và thao tác cơ sở dữ liệu quan hệ. Dùng cho quản lý cơ sở dữ liệu, truy xuất, thao tác và định nghĩa dữ liệu."
67,Giải thích sự khác biệt giữa cơ sở dữ liệu SQL và NoSQL.,"SQL: Cơ sở dữ liệu quan hệ, lưu trữ dữ liệu có cấu trúc schema với bảng, hàng, cột. Schema cố định, cần định trước. Ngôn ngữ truy vấn SQL chuẩn hỗ trợ join, tổng hợp. NoSQL: Mô hình dữ liệu khác (tài liệu JSON/BSON, key-value, cột gia đình, đồ thị). Schema động hoặc không, chèn dữ liệu linh hoạt. Ngôn ngữ truy vấn tùy chỉnh theo mô hình."
68,Các hệ quản lý cơ sở dữ liệu SQL chính (DBMS) là gì?,"Các DBMS SQL phổ biến: MySQL, Microsoft SQL Server, SQLite, PostgreSQL, Oracle Database, Amazon RDS."
69,Mô hình ER trong SQL là gì?,"Mô hình ER (Entity-Relationship) là khung khái niệm đại diện cấu trúc và quan hệ giữa thực thể dữ liệu trong cơ sở dữ liệu. Thường dùng với SQL để tạo cấu trúc cơ sở dữ liệu quan hệ, dù không phải phần của SQL."
70,Biến đổi dữ liệu (Data Transformation) là gì?,"Biến đổi dữ liệu là quá trình chuyển dữ liệu từ cấu trúc, định dạng hoặc biểu diễn này sang cái khác để phù hợp mục tiêu như phân tích, hình dung, báo cáo hoặc lưu trữ. Bao gồm các hành động như tích hợp, làm sạch, thường là bước trong chuẩn bị dữ liệu."
71,Các thành phần chính của truy vấn SQL là gì?,"Các thành phần: SELECT (chọn cột), FROM (nguồn bảng), WHERE (lọc hàng), GROUP BY (nhóm hàng), HAVING (lọc nhóm), ORDER BY (sắp xếp), LIMIT (giới hạn hàng), JOIN (kết hợp bảng)."
72,Khóa chính (Primary Key) là gì?,"Khóa chính là cột trong bảng cơ sở dữ liệu quan hệ duy nhất cho mỗi bản ghi, định danh duy nhất. Phải duy nhất, không null cho mọi hàng."
73,Mục đích của mệnh đề GROUP BY là gì và cách sử dụng nó?,"GROUP BY dùng để tạo hàng tóm tắt từ hàng có giá trị giống ở cột chỉ định. Thường dùng với hàm tổng hợp (SUM, COUNT, AVG) để tính trên nhóm, tạo báo cáo tóm tắt và phân tích sâu."
74,Mệnh đề WHERE dùng để làm gì và cách dùng nó để lọc dữ liệu?,"WHERE lọc hàng từ bảng hoặc kết quả dựa trên tiêu chí. Chọn chỉ hàng thỏa mãn điều kiện hoặc mẫu, dùng cho truy xuất và thao tác dữ liệu."
75,Bạn truy xuất giá trị duy nhất từ cột trong SQL như thế nào?,"Dùng DISTINCT với SELECT để lấy giá trị duy nhất từ cột, loại bỏ trùng lặp."
76,Mệnh đề HAVING là gì?,"HAVING lọc kết quả truy vấn dựa trên đầu ra hàm tổng hợp, dùng với GROUP BY. Lọc nhóm sau khi nhóm, khác WHERE lọc trước nhóm."
77,Bạn xử lý giá trị thiếu hoặc NULL trong bảng cơ sở dữ liệu như thế nào?,"Giá trị NULL do nhập liệu không đầy đủ, trường tùy chọn. Xử lý: Thay NULL bằng giá trị placeholder. Xử lý trong truy vấn (IS NULL, COALESCE). Dùng giá trị mặc định."
78,Giải thích khái niệm chuẩn hóa (Normalization) trong thiết kế cơ sở dữ liệu.,"Chuẩn hóa là phương pháp tổ chức dữ liệu hiệu quả bằng cách giảm trùng lặp và tăng tính toàn vẹn, chia bảng lớn thành bảng nhỏ liên kết, giữ quan hệ. Mục tiêu giảm bất thường chèn, cập nhật, xóa."
79,Denormalization cơ sở dữ liệu là gì?,"Denormalization là giới thiệu trùng lặp có chủ ý bằng cách hợp nhất bảng hoặc thêm dữ liệu dư thừa để tăng hiệu suất truy vấn. Ưu tiên tốc độ đọc bằng giảm join, nhưng có thể dẫn đến không nhất quán và bảo trì phức tạp. Dùng khi đọc nhiều hơn viết."
80,Định nghĩa các loại hàm SQL khác nhau.,"Các loại: Scalar (trả giá trị đơn từ giá trị đầu vào). Aggregate (SUM, COUNT trên nhóm). Window (tính trên tập liên quan, giữ hàng gốc như ROW_NUMBER). Table-Valued (trả bảng). System (hệ thống như GETDATE). User-Defined (tùy chỉnh). Conversion (chuyển kiểu). Conditional (IF, CASE)."
81,Giải thích sự khác biệt giữa INNER JOIN và LEFT JOIN.,INNER JOIN: Trả hàng khớp ở cột chỉ định giữa hai bảng. LEFT JOIN: Trả tất cả hàng từ bảng trái và hàng khớp từ phải; không khớp thì NULL ở cột phải. INNER dùng khi cần dữ liệu khớp cả hai. LEFT đảm bảo mọi hàng bảng trái xuất hiện.
82,Hàm cửa sổ (Window Functions) trong SQL là gì và chúng khác hàm tổng hợp thông thường như thế nào?,"Window Functions: Tính trên tập hàng liên quan đến hàng hiện tại, giữ hàng gốc (ví dụ chạy tổng với OVER). Ví dụ: ROW_NUMBER, RANK, SUM OVER. Khác aggregate: Aggregate nhóm và trả một kết quả mỗi nhóm; window tính qua hàng nhưng giữ chi tiết hàng cá nhân."
83,Bạn thực hiện tính toán toán học trong truy vấn SQL như thế nào?,"Dùng toán tử số học (+, -, *, /) và hàm toán (SQRT, POWER). Hàm tổng hợp (SUM, AVG). Biểu thức tùy chỉnh (cột * hằng số)."
84,Sự khác biệt giữa JOIN và SUBQUERY trong SQL là gì và khi nào dùng mỗi cái?,"JOIN: Kết hợp dữ liệu từ nhiều bảng dựa cột chung (INNER lấy khớp, LEFT lấy tất cả trái). SUBQUERY: Truy vấn lồng, dùng để lấy giá trị/set giá trị cho truy vấn ngoài (ví dụ tìm khách chi > lượng nhất định). Dùng JOIN khi kết hợp trực tiếp; SUBQUERY khi cần giá trị trung gian."
85,Sự khác biệt giữa Cơ sở dữ liệu (Database) và Kho dữ liệu (Data Warehouse) là gì?,"Database: Ưu tiên nhất quán và xử lý thời gian thực, tối ưu lưu trữ/truy xuất/quản lý dữ liệu có cấu trúc. Dùng cho chức năng hành chính như xử lý đơn hàng. Data Warehouse: Cho phân tích, lưu trữ/xử lý dữ liệu lịch sử lớn từ nhiều nguồn. Dùng cho BI, phân tích, quyết định."
86,Giải thích hoạt động tích chập (convolution) của kiến trúc CNN.,"Trong CNN, tích chập giúp tìm đặc trưng quan trọng trong ảnh như cạnh hoặc kết cấu. Bộ lọc nhỏ (kernel) trượt qua ảnh, tính toán từng phần tạo bản đồ đặc trưng. Stride kiểm soát bước di chuyển. Sau tích chập, pooling thu nhỏ bản đồ, giữ chi tiết quan trọng, giảm dữ liệu. Giúp CNN nhận dạng mẫu bất kể vị trí."
87,Mạng nơ-ron tiến (Feed Forward Network) là gì và nó khác Mạng nơ-ron hồi quy (Recurrent Neural Network) như thế nào?,"Feed Forward NN: Dữ liệu chảy một chiều từ đầu vào đến đầu ra, không vòng lặp. Gồm lớp đầu vào, ẩn, đầu ra. Dùng cho phân loại/hồi quy với đầu vào kích thước cố định. RNN: Xử lý dữ liệu tuần tự, có kết nối vòng lặp giữ trạng thái ẩn mang thông tin từ bước trước. Phù hợp NLP, chuỗi thời gian, nhưng gặp vấn đề gradient biến mất với phụ thuộc dài."
88,Giải thích sự khác biệt giữa mô hình sinh (generative) và phân biệt (discriminative)?,"Mô hình sinh: Học cách dữ liệu được sinh, quan hệ giữa X và Y, tạo dữ liệu mới giống gốc. Dùng tạo ảnh/văn bản (GANs, VAEs). Mô hình phân biệt: Phân biệt lớp hoặc dự đoán dựa X, học trực tiếp X đến Y. Dùng phân loại (Logistic Regression, SVM, CNN)."
89,Tuyên truyền tiến (forward propagation) và lùi (backward propagation) trong deep learning là gì?,"Forward Propagation: Truyền dữ liệu đầu vào qua mạng để tạo dự đoán, từ lớp đầu vào qua ẩn đến đầu ra. Backward Propagation: Sau forward, tính lỗi (so dự đoán-thực), dùng quy tắc chuỗi tính gradient lỗi theo tham số, cập nhật tham số để giảm lỗi tương lai."
90,Mô tả sử dụng mô hình Markov trong phân tích dữ liệu tuần tự?,"Mô hình Markov nắm bắt phụ thuộc giữa điểm dữ liệu liên tiếp. Thuộc tính Markov: Tương lai phụ thuộc trạng thái hiện tại, độc lập quá khứ. Markov Chain: Trạng thái và xác suất chuyển tiếp. Hidden Markov Model: Trạng thái ẩn và phát xạ quan sát. Ứng dụng: Nhận diện giọng nói (mô hình âm vị), genomics (dự đoán gen), tài chính (dự đoán giá cổ phiếu)."
91,AI sinh (Generative AI) là gì?,"Generative AI là lớp hệ thống AI tạo dữ liệu mới độc đáo giống dữ liệu con người. Tập trung sáng tạo, tạo văn bản, hình ảnh, âm thanh. Ví dụ: GPT tạo văn bản; GANs tạo ảnh; WaveGAN tạo âm thanh."
92,Các kiến trúc mạng nơ-ron khác nhau dùng để tạo dữ liệu nhân tạo trong deep learning là gì?,"GANs: Generator và discriminator huấn luyện đối kháng tạo ảnh chất lượng cao. VAEs: Encoder-decoder học ánh xạ xác suất, tạo ảnh/văn bản/âm thanh. RNNs: Tạo chuỗi cho văn bản/giọng nói/nhạc. Transformers: Self-attention cho dịch máy/tóm tắt/tạo ngôn ngữ. Autoencoders: Tái tạo đầu vào, biến thể dùng tạo dữ liệu mới."
93,Kỹ thuật Deep Reinforcement Learning là gì?,"Deep RL kết hợp RL với mạng nơ-ron sâu, cho phép máy học nhiệm vụ phức tạp qua tương tác môi trường như thử-sai. Thành phần: Agent (quyết định), Environment (phản hồi), Reward (hướng dẫn tối đa hóa tích lũy). Ứng dụng: Robot (điều khiển), xe tự lái, khuyến nghị cá nhân hóa."
94,Học chuyển giao (transfer learning) là gì và áp dụng trong deep learning như thế nào?,"Học chuyển giao dùng mô hình huấn luyện trên nhiệm vụ này để giúp nhiệm vụ tương tự khác, tận dụng kiến thức trước để học nhanh hơn. Bước: Trích xuất đặc trưng (dùng lớp pretrained, bỏ lớp dự đoán). Fine-tuning (thêm lớp mới, điều chỉnh trên dữ liệu mục tiêu). Tiết kiệm thời gian, tài nguyên, cải thiện hiệu suất với dữ liệu ít."
95,Sự khác biệt giữa Phát hiện đối tượng (Object Detection) và Phân đoạn hình ảnh (Image Segmentation) là gì?,"Object Detection: Xác định và định vị đối tượng bằng hộp giới hạn và nhãn. Dùng lái xe tự động phát hiện người đi bộ. Image Segmentation: Phân vùng ảnh thành phần, gán nhãn pixel cấp. Dùng phân tích hình ảnh y tế phân định cơ quan/u."
96,Giải thích khái niệm nhúng từ (word embeddings) trong Xử lý ngôn ngữ tự nhiên (NLP).,"Nhúng từ là biểu diễn dày đặc từ/cụm trong vector không gian chiều cao, nắm bắt ngữ nghĩa và ngữ cảnh. Dựa giả thuyết phân bố: Từ tương tự ngữ cảnh có nghĩa tương tự. Kỹ thuật: Bag of Words, Word2Vec, GloVe, TF-IDF, BERT."
97,Mô hình seq2seq là gì?,"Mô hình Sequence-to-Sequence (seq2seq) là kiến trúc mạng nơ-ron xử lý chuỗi đầu vào/đầu ra biến đổi dài. Dùng dịch máy, tóm tắt, hỏi đáp. Gồm encoder (chuyển chuỗi đầu vào thành vector cố định nắm đặc trưng) và decoder (tạo chuỗi đầu ra từ vector, autoregressive)."
98,Mạng nơ-ron nhân tạo (Artificial Neural Networks) là gì?,"Mạng nơ-ron nhân tạo lấy cảm hứng từ não người. Đơn vị tính toán là neuron xử lý và truyền thông tin đến lớp sau. Thành phần: Lớp đầu vào (nhận đặc trưng), lớp ẩn (học mẫu), lớp đầu ra (đầu ra cuối)."
99,Mục đích của mô hình hóa sinh (generative modeling) là gì?,"Mô hình hóa sinh được sử dụng để mô hình hóa cấu trúc cơ bản của dữ liệu và tạo ra các mẫu mới từ phân phối đã học, thường được sử dụng trong các nhiệm vụ như tạo hình ảnh, tạo văn bản và tăng cường dữ liệu."
100,Các kỹ thuật mô hình hóa sinh phổ biến là gì?,"Các kỹ thuật phổ biến bao gồm Bộ mã hóa tự động biến thể (VAEs), Mạng đối kháng sinh (GANs) và Máy Boltzmann hạn chế (RBMs)."
101,Bộ mã hóa tự động biến thể (VAE) là gì?,Bộ mã hóa tự động biến thể (VAE) là một loại mô hình sinh học mã hóa và giải mã dữ liệu vào một không gian ẩn chiều thấp hơn trong khi tối đa hóa một giới hạn dưới biến thể trên likelihood của dữ liệu.
102,Mạng đối kháng sinh (GAN) là gì?,"Mạng đối kháng sinh (GAN) là một loại mô hình sinh bao gồm hai mạng nơ-ron, một mạng sinh và một mạng phân biệt, được huấn luyện đối kháng để tạo ra các mẫu thực tế từ một phân phối đã học."
103,Máy Boltzmann hạn chế (RBM) là gì?,"Máy Boltzmann hạn chế (RBM) là một mạng nơ-ron ngẫu nhiên sinh được sử dụng để giảm chiều, học đặc trưng và lọc cộng tác, dựa trên phân phối Boltzmann và lấy mẫu Gibbs."
104,Mục đích của bản đồ tự tổ chức (SOMs) là gì?,"Bản đồ tự tổ chức (SOMs) là một loại mạng nơ-ron không giám sát được sử dụng để giảm chiều và hình ảnh hóa dữ liệu chiều cao, bằng cách ánh xạ không gian đầu vào lên một lưới chiều thấp hơn trong khi bảo tồn các thuộc tính topological của không gian đầu vào."
105,Sự khác biệt giữa các thuật toán phân cụm sinh và phân biệt là gì?,"Các thuật toán phân cụm sinh mô hình hóa phân phối xác suất của dữ liệu, trong khi các thuật toán phân cụm phân biệt trực tiếp tối ưu hóa một hàm tiêu chí để phân chia dữ liệu thành các cụm."
106,Mục đích của tiền xử lý dữ liệu trong học không giám sát là gì?,"Tiền xử lý dữ liệu được sử dụng để chuẩn bị dữ liệu thô cho phân tích bằng cách làm sạch, chuyển đổi và chuẩn hóa các đặc trưng, có thể cải thiện hiệu suất và hiệu quả của các thuật toán học không giám sát."
107,Các kỹ thuật tiền xử lý dữ liệu phổ biến trong học không giám sát là gì?,"Các kỹ thuật phổ biến bao gồm xử lý giá trị thiếu, chuẩn hóa đặc trưng, mã hóa đặc trưng, giảm chiều và phát hiện ngoại lai."
108,Chuẩn hóa đặc trưng (feature scaling) là gì?,"Chuẩn hóa đặc trưng là quá trình chuẩn hóa hoặc bình thường hóa phạm vi của các biến độc lập hoặc đặc trưng trong dữ liệu thành một tỷ lệ tương tự, có thể cải thiện hiệu suất và hội tụ của các thuật toán học không giám sát."
109,Mã hóa đặc trưng (feature encoding) là gì?,"Mã hóa đặc trưng là quá trình chuyển đổi các đặc trưng phân loại hoặc danh nghĩa thành một biểu diễn số có thể được sử dụng làm đầu vào cho các thuật toán học máy, như mã hóa one-hot và mã hóa nhãn."
110,Phát hiện bất thường (anomaly detection) là gì?,"Phát hiện bất thường, còn được gọi là phát hiện ngoại lai, là quá trình xác định các điểm dữ liệu hoặc quan sát lệch đáng kể so với phần còn lại của tập dữ liệu, có thể chỉ ra lỗi, bất thường hoặc mẫu thú vị."
111,Các kỹ thuật phát hiện bất thường phổ biến là gì?,"Các kỹ thuật phổ biến bao gồm các phương pháp thống kê, phương pháp dựa trên mật độ, phương pháp dựa trên khoảng cách và các phương pháp dựa trên học máy như Isolation Forest và One-Class SVM."
112,Isolation Forest là gì?,"Isolation Forest là một thuật toán học không giám sát để phát hiện bất thường, tách biệt các bất thường bằng cách phân chia tập dữ liệu một cách đệ quy thành các tập con sử dụng các phân tách ngẫu nhiên, làm cho các bất thường dễ bị cô lập hơn các điểm dữ liệu bình thường."
113,One-Class SVM là gì?,"One-Class Support Vector Machine (SVM) là một thuật toán học máy để phát hiện bất thường, học một ranh giới xung quanh các điểm dữ liệu bình thường trong không gian đặc trưng và xác định các bất thường là các điểm dữ liệu nằm ngoài ranh giới này."
114,Mục đích của ước lượng mật độ (density estimation) là gì?,"Ước lượng mật độ là quá trình ước lượng hàm mật độ xác suất của một biến ngẫu nhiên từ một tập hợp các điểm dữ liệu, thường được sử dụng trong phân cụm, phát hiện bất thường và mô hình hóa sinh."
115,Các kỹ thuật ước lượng mật độ phổ biến là gì?,"Các kỹ thuật phổ biến bao gồm các phương pháp dựa trên biểu đồ tần số, ước lượng mật độ hạt nhân (KDE), mô hình hỗn hợp Gaussian (GMM) và ước lượng cửa sổ Parzen."
116,Mô hình hỗn hợp Gaussian (GMM) là gì?,"Mô hình hỗn hợp Gaussian (GMM) là một mô hình xác suất được sử dụng để ước lượng mật độ và phân cụm, giả định rằng dữ liệu được tạo ra từ một hỗn hợp của một số phân phối Gaussian."
117,"Thuật toán Tối đa hóa Kỳ vọng (Expectation-Maximization, EM) là gì?","Thuật toán Tối đa hóa Kỳ vọng (EM) là một thuật toán tối ưu hóa lặp được sử dụng để ước lượng các tham số của các mô hình xác suất với các biến ẩn, như mô hình hỗn hợp Gaussian (GMM), bằng cách lặp đi lặp lại tính toán các giá trị kỳ vọng của các biến ẩn và tối đa hóa hàm likelihood."
118,Học tăng cường (reinforcement learning) là gì?,"Học tăng cường là một loại học máy nơi một tác nhân học cách đưa ra quyết định bằng cách tương tác với một môi trường, nhận phản hồi dưới dạng phần thưởng hoặc hình phạt."
119,Các thành phần chính của học tăng cường là gì?,"Các thành phần chính bao gồm tác nhân, môi trường, hành động, trạng thái, phần thưởng và chính sách."
120,Tác nhân (agent) trong học tăng cường là gì?,"Tác nhân là thực thể học cách đưa ra quyết định trong bối cảnh học tăng cường, thường được biểu diễn bởi một thuật toán hoặc chương trình."
121,Môi trường (environment) trong học tăng cường là gì?,"Môi trường là hệ thống hoặc quy trình bên ngoài mà tác nhân tương tác, và nó xác định các trạng thái có thể của tác nhân và kết quả của các hành động của nó."
122,Trạng thái (state) trong học tăng cường là gì?,"Trạng thái là một biểu diễn của tình huống hoặc cấu hình hiện tại của môi trường, cung cấp thông tin cần thiết để tác nhân đưa ra quyết định."
123,Hành động (action) trong học tăng cường là gì?,Hành động là một quyết định hoặc lựa chọn được thực hiện bởi tác nhân ảnh hưởng đến trạng thái của môi trường và chuyển tác nhân sang một trạng thái mới.
124,Phần thưởng (reward) trong học tăng cường là gì?,"Phần thưởng là một tín hiệu số được cung cấp bởi môi trường cho tác nhân để đánh giá mức độ tốt hoặc mong muốn của các hành động của nó, với mục tiêu tối đa hóa phần thưởng tích lũy theo thời gian."
125,Chính sách (policy) trong học tăng cường là gì?,"Chính sách là chiến lược hoặc quy tắc mà tác nhân sử dụng để chọn hành động trong một trạng thái nhất định, ánh xạ các trạng thái đến các hành động hoặc phân phối xác suất trên các hành động."
126,Thăm dò (exploration) trong học tăng cường là gì?,Thăm dò là quá trình chủ động tìm kiếm các trạng thái hoặc hành động mới hoặc chưa biết để cải thiện sự hiểu biết của tác nhân về môi trường và có khả năng khám phá các chính sách tốt hơn.
127,Khai thác (exploitation) trong học tăng cường là gì?,Khai thác là quá trình chọn các hành động mà tác nhân tin rằng sẽ dẫn đến phần thưởng tức thời cao nhất dựa trên kiến thức hoặc chính sách hiện tại của nó.
128,Sự đánh đổi giữa thăm dò và khai thác trong học tăng cường là gì?,"Sự đánh đổi giữa thăm dò và khai thác đề cập đến tình huống khó xử mà các tác nhân phải đối mặt trong việc cân bằng giữa nhu cầu thăm dò các lựa chọn mới để khám phá các chính sách tối ưu với mong muốn khai thác các lựa chọn đã biết để nhận phần thưởng tức thời, nhằm đạt được hiệu suất tối ưu lâu dài."
129,Các thuật toán phổ biến được sử dụng trong học tăng cường là gì?,"Các thuật toán phổ biến bao gồm Q-learning, SARSA, Mạng Q sâu (DQN), phương pháp gradient chính sách và phương pháp Diễn viên-Nhà phê bình (Actor-Critic)."
130,Q-learning là gì?,Q-learning là một thuật toán học tăng cường không mô hình học hàm giá trị hành động tối ưu (hàm Q) bằng cách cập nhật lặp đi lặp lại các ước lượng của giá trị Q dựa trên phần thưởng và chuyển đổi được quan sát.
131,SARSA là gì?,SARSA là một thuật toán học tăng cường không mô hình tương tự như Q-learning nhưng cập nhật hàm giá trị hành động (hàm Q) dựa trên phần thưởng và chuyển đổi được quan sát cho cặp trạng thái-hành động hiện tại và cặp trạng thái-hành động tiếp theo.
132,Mạng Q sâu (DQN) là gì?,"Mạng Q sâu (DQN) là một lớp thuật toán học tăng cường sử dụng mạng nơ-ron sâu để xấp xỉ hàm giá trị hành động (hàm Q), cho phép học từ các đầu vào cảm giác chiều cao như hình ảnh."
133,Phương pháp gradient chính sách (Policy Gradient methods) là gì?,"Phương pháp gradient chính sách là một lớp thuật toán học tăng cường học trực tiếp hàm chính sách, thường sử dụng tăng gradient trên một chính sách tham số hóa để tối đa hóa phần thưởng tích lũy kỳ vọng."
134,Phương pháp Diễn viên-Nhà phê bình (Actor-Critic methods) là gì?,Phương pháp Diễn viên-Nhà phê bình là một lớp thuật toán học tăng cường kết hợp lợi ích của phương pháp gradient chính sách (diễn viên) và phương pháp dựa trên giá trị (nhà phê bình) bằng cách học đồng thời một chính sách và một hàm giá trị.
135,Phương trình Bellman là gì?,"Phương trình Bellman là một phương trình cơ bản trong học tăng cường phân tách giá trị của một trạng thái thành phần thưởng tức thời cộng với giá trị chiết khấu của trạng thái tiếp theo, tạo cơ sở cho ước lượng giá trị lặp và cải thiện chính sách."
136,Quy trình ra quyết định Markov (MDP) là gì?,"Quy trình ra quyết định Markov (MDP) là một khung toán học được sử dụng để mô hình hóa các vấn đề ra quyết định tuần tự trong học tăng cường, bao gồm các trạng thái, hành động, xác suất chuyển đổi và phần thưởng, với tính chất Markov."
137,Tính chất Markov là gì?,"Tính chất Markov nói rằng trạng thái tương lai của một hệ thống chỉ phụ thuộc vào trạng thái hiện tại và hành động được thực hiện, độc lập với lịch sử trước đó của các trạng thái và hành động, làm cho nó trở thành một thuộc tính không nhớ."
138,Hàm giá trị (value function) trong học tăng cường là gì?,"Hàm giá trị là một hàm ước lượng phần thưởng tích lũy kỳ vọng hoặc tiện ích của việc ở trong một trạng thái nhất định hoặc thực hiện một hành động nhất định trong một vấn đề học tăng cường, định hướng quá trình ra quyết định của tác nhân."
139,Thuật toán lặp chính sách (policy iteration) là gì?,"Lặp chính sách là một thuật toán lặp để tìm chính sách tối ưu trong học tăng cường, xen kẽ giữa đánh giá chính sách (ước lượng hàm giá trị cho một chính sách nhất định) và cải thiện chính sách (cập nhật chính sách để tham lam đối với hàm giá trị hiện tại)."
140,Thuật toán lặp giá trị (value iteration) là gì?,"Lặp giá trị là một thuật toán lặp để tìm hàm giá trị tối ưu trong học tăng cường, lặp đi lặp lại áp dụng toán tử sao lưu Bellman để cập nhật các ước lượng giá trị cho đến khi hội tụ."
141,Học sai lệch thời gian (temporal difference learning) là gì?,"Học sai lệch thời gian (TD) là một phương pháp học được sử dụng trong học tăng cường cập nhật các ước lượng giá trị dựa trên sự khác biệt giữa giá trị ước lượng của trạng thái hiện tại và giá trị ước lượng của trạng thái tiếp theo, dựa trên các ước lượng hiện tại."
142,Học Monte Carlo trong học tăng cường là gì?,"Học Monte Carlo trong học tăng cường là một phương pháp học ước lượng hàm giá trị bằng cách lấy trung bình phần thưởng tích lũy thu được từ các tập hợp hoàn chỉnh (quỹ đạo) của tương tác với môi trường, mà không dựa vào hoặc giả định kiến thức về động lực của môi trường."
143,Dấu vết đủ điều kiện (eligibility trace) là gì?,"Dấu vết đủ điều kiện là một cơ chế được sử dụng trong các thuật toán học tăng cường, như TD(λ), để phân bổ tín dụng hoặc trách nhiệm cho các trạng thái và hành động trước đó dựa trên đóng góp của chúng vào phần thưởng được quan sát, cho phép phân bổ tín dụng và học tập hiệu quả hơn."
144,Xấp xỉ hàm (function approximation) trong học tăng cường là gì?,"Xấp xỉ hàm là việc sử dụng các hàm tham số hóa, như mạng nơ-ron, để biểu diễn các hàm giá trị, chính sách hoặc các thành phần khác của các thuật toán học tăng cường, cho phép học từ các không gian trạng thái và hành động chiều cao và liên tục."
145,Học off-policy là gì?,Học off-policy là một phương pháp học trong học tăng cường nơi tác nhân học hàm giá trị hoặc chính sách dựa trên các trải nghiệm thu thập từ một chính sách hành vi khác (có thể ngẫu nhiên) so với chính sách đang được cập nhật.
146,Học on-policy là gì?,"Học on-policy là một phương pháp học trong học tăng cường nơi tác nhân học hàm giá trị hoặc chính sách dựa trên các trải nghiệm thu thập từ cùng chính sách mục tiêu đang được cập nhật, thường sử dụng các trải nghiệm được thu thập theo cách lấy mẫu hoặc tuần tự."
147,Hàm lợi thế (advantage function) trong học tăng cường là gì?,"Hàm lợi thế là một hàm được sử dụng trong các phương pháp diễn viên-nhà phê bình để ước lượng lợi thế của việc thực hiện một hành động cụ thể trong một trạng thái nhất định so với giá trị trung bình của tất cả các hành động trong trạng thái đó, giúp định hướng cập nhật chính sách."
148,Tình huống khó xử thăm dò-khai thác trong học tăng cường là gì?,"Tình huống khó xử thăm dò-khai thác đề cập đến thách thức mà các tác nhân phải đối mặt trong việc cân bằng giữa nhu cầu thăm dò các hành động hoặc trạng thái mới để khám phá các chính sách tối ưu với mong muốn khai thác các hành động hoặc trạng thái đã biết để nhận phần thưởng tức thời, nhằm đạt được hiệu suất tối ưu lâu dài."
149,Hệ số chiết khấu (discount factor) trong học tăng cường là gì?,"Hệ số chiết khấu là một tham số được sử dụng trong các thuật toán học tăng cường để chiết khấu các phần thưởng tương lai so với phần thưởng tức thời, đại diện cho sở thích của tác nhân đối với phần thưởng tức thời hơn phần thưởng bị trì hoãn và ảnh hưởng đến quá trình ra quyết định thời gian của tác nhân."
150,Vai trò của phần thưởng trong học tăng cường là gì?,"Phần thưởng trong học tăng cường đóng vai trò là tín hiệu cung cấp phản hồi cho tác nhân về mức độ mong muốn hoặc tốt đẹp của các hành động của nó, định hướng quá trình học tập của tác nhân và ảnh hưởng đến các quyết định chính sách của nó."
151,Gradient chính sách trong học tăng cường là gì?,"Gradient chính sách là một phương pháp được sử dụng để cập nhật các tham số chính sách trong các thuật toán học tăng cường học trực tiếp một chính sách, thường bằng cách tính gradient của một mục tiêu hiệu suất (ví dụ: phần thưởng tích lũy kỳ vọng) đối với các tham số chính sách và thực hiện tăng gradient."
152,Lợi thế của học tăng cường sâu là gì?,"Học tăng cường sâu kết hợp lợi ích của các thuật toán học tăng cường với mạng nơ-ron sâu, cho phép học từ các đầu vào cảm giác chiều cao và không gian trạng thái và hành động phức tạp, cho phép học tập có khả năng mở rộng và linh hoạt hơn."
153,Một số ứng dụng của học tăng cường là gì?,"Học tăng cường được áp dụng trong các lĩnh vực khác nhau, bao gồm robot, xe tự hành, chơi game, hệ thống gợi ý, tài chính, y tế và xử lý ngôn ngữ tự nhiên."
154,Tìm kiếm chính sách (policy search) trong học tăng cường là gì?,"Tìm kiếm chính sách là một cách tiếp cận học tăng cường tối ưu hóa trực tiếp hàm chính sách, thường sử dụng các kỹ thuật tối ưu hóa như gradient descent hoặc thuật toán tiến hóa để tìm kiếm các tham số chính sách tốt nhất."
