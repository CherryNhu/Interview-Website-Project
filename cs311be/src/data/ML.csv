STT,Câu Hỏi,Câu Trả Lời
1,Sự đánh đổi giữa bias và variance là gì?,"Nếu mô hình quá đơn giản với ít tham số thì có thể có bias cao và variance thấp. Ngược lại, nếu mô hình có nhiều tham số thì sẽ có variance cao và bias thấp. Chúng ta cần tìm sự cân bằng tốt mà không bị overfitting và underfitting."
2,Gradient descent là gì?,Gradient descent là thuật toán tối ưu hóa được sử dụng để tìm giá trị của các tham số (hệ số) của hàm số (f) để tối thiểu hóa hàm chi phí (cost). Gradient descent được sử dụng tốt nhất khi các tham số không thể được tính toán một cách phân tích và phải được tìm kiếm bằng thuật toán tối ưu hóa.
3,"Giải thích overfitting và underfitting, cách khắc phục?","Underfitting xảy ra khi tính linh hoạt của mô hình không đủ để nắm bắt mẫu cơ bản trong tập dữ liệu huấn luyện. Overfitting xảy ra khi mô hình quá linh hoạt với mẫu cơ bản, được gọi là mô hình đã ""ghi nhớ"" dữ liệu huấn luyện."
5,"Regularization là gì, tại sao sử dụng và một số phương pháp phổ biến?","Kỹ thuật ngăn cản việc học một mô hình phức tạp hoặc linh hoạt hơn để tránh rủi ro overfitting. Các ví dụ:<br>- Ridge (L2 norm)<br>- Lasso (L1 norm)<br>Lasso có thể làm một số hệ số bằng 0, thực hiện việc lựa chọn biến và tạo ra mô hình thưa."
6,Giải thích Phân tích thành phần chính (PCA)?,PCA là kỹ thuật giảm chiều được sử dụng trong machine learning để giảm số lượng đặc trưng trong tập dữ liệu trong khi giữ lại nhiều thông tin nhất có thể. Nó hoạt động bằng cách xác định các hướng (thành phần chính) mà dữ liệu thay đổi nhiều nhất.
7,Tại sao ReLU tốt hơn và được sử dụng nhiều hơn Sigmoid trong Mạng Neural?,"- Hiệu quả tính toán: ReLU đơn giản nên đường tiến và lùi nhanh hơn<br>- Giảm khả năng vanishing gradient: Gradient của ReLU là 1 cho giá trị dương và 0 cho giá trị âm<br>- Tính thưa: Xảy ra khi đầu vào của ReLU âm, ít neuron hoạt động hơn"
8,Tạo hàm tính receptive field của một node trong CNN?,"Receptive field là phần không gian trong đầu vào được sử dụng để tạo ra đầu ra. Với bộ lọc CNN kích thước k, receptive field là k nhân với chiều của đầu vào không bị giảm bởi bộ lọc tích chập."
12,Làm thế nào để loại bỏ outliers khi ước lượng mặt phẳng từ mẫu nhiễu?,"RANSAC (Random sample consensus) là phương pháp lặp để ước lượng tham số của mô hình toán học từ dữ liệu quan sát có chứa outliers, khi outliers không được tính đến trong ước lượng."
13,CBIR hoạt động như thế nào?,"Content-based image retrieval sử dụng hình ảnh để thu thập metadata về nội dung của chúng. So với phương pháp truy xuất dựa trên từ khóa, kỹ thuật này tạo metadata từ các kỹ thuật computer vision để trích xuất thông tin liên quan."
14,Image registration hoạt động như thế nào?,[Cần giải thích về Sparse vs dense optical flow]
15,Mô tả cách convolution hoạt động với ảnh grayscale vs RGB?,"Trong CNN, phép tích chập được áp dụng lên ảnh đầu vào sử dụng kernel nhỏ. Kernel trượt qua ảnh với các bước nhỏ (strides), thực hiện phép nhân element-wise và cộng kết quả. Với RGB, sliding window sẽ là sliding cube. Hình dạng lớp tiếp theo được xác định bởi kích thước kernel, số kernel, stride, padding và dilation."
16,Tạo mô hình 3D của vật thể từ ảnh và sensor độ sâu?,Hai phương pháp phổ biến:<br>- Structure from Motion (SfM): tốt cho cảnh lớn<br>- Multi-View Stereo (MVS): tốt cho vật thể nhỏ
17,Cài đặt SQRT(x) không dùng hàm đặc biệt?,Có thể sử dụng chuỗi Taylor để xấp xỉ sqrt(x)
18,Đảo ngược bitstring,Trong Python3:<br>data = b'\xAD\xDE\xDE\xC0'<br>my_data = bytearray(data)<br>my_data.reverse()
19,Cài đặt non-maximal suppression hiệu quả,"NMS loại bỏ nhiều detection của cùng một object. Sắp xếp bounding boxes theo điểm số (N LogN), loại bỏ boxes có IoU > ngưỡng (N^2). Có thể tối ưu bằng R-tree hoặc KD-tree (N LogN)."
21,Data normalization là gì và tại sao cần?,"Bước tiền xử lý quan trọng để rescale giá trị vào khoảng cụ thể đảm bảo hội tụ tốt hơn. Thường là trừ mean và chia cho standard deviation. Nếu không làm, các đặc trưng có độ lớn cao sẽ được weighted nhiều hơn trong cost function."
22,Tại sao dùng convolutions cho ảnh thay vì FC layers?,"- Convolutions bảo tồn, mã hóa và sử dụng thông tin không gian từ ảnh<br>- CNN có tính bất biến translation một phần, vì mỗi convolution kernel hoạt động như bộ lọc/detector đặc trưng riêng"
23,Điều gì làm CNN bất biến translation?,"Mỗi convolution kernel hoạt động như bộ lọc riêng. Trong object detection, vị trí object không quan trọng vì convolution được áp dụng theo sliding window trên toàn bộ ảnh."
24,Tại sao có max-pooling trong CNN classification?,Max-pooling giảm tính toán vì feature maps nhỏ hơn sau pooling. Không mất nhiều thông tin semantic vì lấy activation tối đa. Cũng góp phần tăng tính bất biến translation cho CNN.
25,Tại sao CNN segmentation thường có cấu trúc encoder-decoder?,"Encoder CNN được coi như mạng trích xuất đặc trưng, decoder sử dụng thông tin đó để dự đoán segments bằng cách ""decode"" đặc trưng và upscale về kích thước ảnh gốc."
26,Ý nghĩa của Residual Networks?,"Residual connections cho phép truy cập đặc trưng trực tiếp từ các lớp trước, làm cho việc truyền thông tin qua mạng dễ dàng hơn. Tạo cấu trúc ensemble multi-path, cho đặc trưng nhiều đường để lan truyền."
27,Batch normalization là gì và tại sao hoạt động?,Huấn luyện DNN phức tạp vì phân phối đầu vào mỗi lớp thay đổi khi tham số các lớp trước thay đổi. Ý tưởng là chuẩn hóa đầu vào mỗi lớp có mean = 0 và std = 1 cho từng mini-batch.
28,Tại sao dùng nhiều kernel nhỏ 3x3 thay vì ít kernel lớn?,"Hai lý do từ VGGNet: 1) Dùng nhiều kernel nhỏ để có cùng receptive field nhưng ít tham số và tính toán hơn. 2) Với kernel nhỏ hơn sử dụng nhiều filter và activation functions hơn, có mapping function discriminative hơn."
29,Tại sao cần validation set và test set?,"Training set: fit tham số mô hình<br>Validation set: đo hiệu suất trên dữ liệu không trong training, tune hyperparameters<br>Test set: đo hiệu suất trên dữ liệu chưa thấy, chỉ dùng một lần sau khi tune bằng validation set"
30,Stratified cross-validation là gì và khi nào dùng?,"Cross-validation bảo tồn tỷ lệ các categories trên cả training và validation sets. Dùng khi: dataset có nhiều categories, dataset nhỏ và categories không cân bằng, dữ liệu có phân phối khác nhau."
31,Tại sao ensembles thường có điểm cao hơn mô hình đơn lẻ?,Ensemble kết hợp nhiều mô hình để tạo dự đoán duy nhất. Ý tưởng chính là các mô hình nên tạo ra lỗi khác nhau để lỗi của mô hình này được bù trừ bởi dự đoán đúng của mô hình khác.
32,Dataset không cân bằng là gì? Cách xử lý?,"Dataset có tỷ lệ target categories khác nhau. Cách xử lý:<br>- Oversampling hoặc undersampling<br>- Data augmentation<br>- Sử dụng metrics phù hợp (precision, recall, F-score)"
33,"Khác biệt giữa supervised, unsupervised và reinforcement learning?","Supervised: học mối quan hệ input-output với labeled data<br>Unsupervised: chỉ có unlabeled data, học representation<br>Reinforcement: có input và reward, học policy tối đa hóa reward"
34,Data augmentation là gì? Ví dụ?,"Kỹ thuật tổng hợp dữ liệu mới bằng cách sửa đổi dữ liệu hiện có mà không thay đổi target. Ví dụ cho ảnh: Resize, Flip, Rotate, Add noise, Deform, Modify colors"
35,Turing test là gì?,"Phương pháp kiểm tra khả năng của máy để đạt trình độ thông minh như con người. Máy được dùng để thách thức trí thông minh con người, khi vượt qua được coi là thông minh."
36,Precision là gì?,Precision là tỷ lệ instances liên quan trong số các instances được truy xuất.<br>Precision = true positive / (true positive + false positive)
37,Recall là gì?,Recall là tỷ lệ instances liên quan được truy xuất trên tổng số instances liên quan.<br>Recall = true positive / (true positive + false negative)
38,Định nghĩa F1-score,"Trung bình có trọng số của precision và recall, xem xét cả false positive và false negative.<br>F1-Score = 2 * (precision * recall) / (precision + recall)"
39,Cost function là gì?,"Hàm scalar định lượng error factor của Neural Network. Cost function càng thấp, Neural network càng tốt."
40,Liệt kê các activation functions,"Linear Neuron, Binary Threshold Neuron, Stochastic Binary Neuron, Sigmoid Neuron, Tanh function, Rectified Linear Unit (ReLU)"
41,Định nghĩa Learning Rate,Learning rate là hyper-parameter điều khiển mức độ điều chỉnh weights của mạng theo loss gradient.
42,Momentum trong NN optimization là gì?,Momentum cho phép thuật toán tối ưu nhớ bước cuối và thêm một tỷ lệ vào bước hiện tại. Giúp thoát khỏi vùng phẳng hoặc local minimum nhỏ.
43,Khác biệt Batch GD và Stochastic GD?,"Batch GD: tính gradient sử dụng toàn bộ dataset<br>SGD: tính gradient sử dụng một sample duy nhất, hoạt động tốt với error manifolds có nhiều local maxima/minima"
44,Epoch vs Batch vs Iteration,Epoch: một forward pass và backward pass của tất cả training examples<br>Batch: examples được xử lý cùng nhau trong một pass<br>Iteration: số training examples / Batch size
45,Vanishing gradient là gì?,"Khi thêm nhiều hidden layers, back propagation trở nên ít hữu ích trong việc truyền thông tin đến các lớp thấp hơn. Gradients bắt đầu biến mất và trở nên nhỏ so với weights."
46,Dropouts là gì?,"Cách đơn giản để ngăn neural network overfitting bằng cách dropping out một số units trong mạng, tương tự quá trình sinh sản tự nhiên."
47,Định nghĩa LSTM,Long Short Term Memory - được thiết kế để giải quyết vấn đề long term dependency bằng cách duy trì state về những gì cần nhớ và quên.
48,Các thành phần chính của LSTM,"Gates (forget, Memory, update & Read), tanh(x) (giá trị -1 đến 1), Sigmoid(x) (giá trị 0 đến 1)"
49,Các biến thể của RNN,LSTM: Long Short Term Memory<br>GRU: Gated Recurrent Unit<br>End to End Network<br>Memory Network
50,"Autoencoder là gì, ứng dụng?","Học dạng nén của dữ liệu cho trước. Ứng dụng: Data denoising, Dimensionality reduction, Image reconstruction, Image colorization"
51,Các thành phần của GAN?,Generator và Discriminator
52,Khác biệt giữa boosting và bagging?,"Cả hai đều là kỹ thuật ensembling. Bagging: lấy bootstrap samples và mỗi sample train một weak learner. Boosting: sử dụng tất cả dữ liệu cho mỗi learner, instances bị misclassified được cho trọng số cao hơn."
53,Giải thích ROC curve hoạt động,ROC curve là biểu diễn đồ họa của tỷ lệ true positive và false positive ở các ngưỡng khác nhau. Thường được dùng như proxy cho trade-off giữa sensitivity và probability of false alarm.
54,Khác biệt Type I và Type II error?,Type I error: false positive (claim something happened when it hasn't)<br>Type II error: false negative (claim nothing happening when something is)
55,Khác biệt generative và discriminative model?,Generative model: học categories của dữ liệu<br>Discriminative model: học sự phân biệt giữa categories khác nhau. Discriminative thường outperform generative trong classification tasks.
56,Instance-Based vs Model-Based Learning,"Instance-based: hệ thống học examples bằng trái tim, tổng quát hóa với similarity measure<br>Model-based: xây dựng model từ examples, dùng model để dự đoán"
57,Khi nào dùng Label Encoding vs One Hot Encoding?,"One-Hot: khi categorical feature không ordinal, số categories ít<br>Label Encoding: khi categorical feature là ordinal, số categories lớn"
58,Khác biệt LDA và PCA cho dimensionality reduction?,"Cả hai là kỹ thuật linear transformation. LDA là supervised, PCA là unsupervised. PCA tìm hướng variance tối đa, LDA tìm feature subspace tối đa hóa class separability."
59,t-SNE là gì?,"t-Distributed Stochastic Neighbor Embedding là kỹ thuật unsupervised, non-linear chủ yếu cho data exploration và visualizing high-dimensional data."
60,Khác biệt t-SNE và PCA?,"PCA (1933) là linear, tối đa hóa variance, bảo tồn large pairwise distances. t-SNE (2008) chỉ bảo tồn small pairwise distances hoặc local similarities."
61,UMAP là gì?,"Uniform Manifold Approximation and Projection là kỹ thuật manifold learning mới cho dimension reduction, dựa trên Riemannian geometry và algebraic topology."
62,Khác biệt t-SNE và UMAP?,"UMAP tốt hơn trong việc bảo tồn global structure, inter-cluster relations có ý nghĩa hơn t-SNE. Nhưng cả hai đều warp high-dimensional shape khi project xuống low dimensions."
63,Random Number Generator hoạt động như thế nào?,Tạo ra pseudo random number dựa trên seed và có các thuật toán nổi tiếng.
64,Vấn đề với splitting mechanism khi evaluate n models?,"rand() function sắp xếp dữ liệu khác nhau mỗi lần chạy, vì vậy 80% rows sẽ khác nhau mỗi lần. Cần set random seed hoặc dùng random_state parameter để có consistent sampling."
65,Khác biệt Bayesian vs Frequentist statistics?,"Frequentist: tập trung estimate population parameters bằng sample statistics<br>Bayesian: sử dụng prior knowledge để update beliefs về parameter, cung cấp probability distributions cho parameters"
66,Khác biệt cơ bản LSTM và Transformers?,LSTM: RNN cells để store và manipulate thông tin across time steps<br>Transformers: stack của encoder-decoder layers với self attention và feed-forward neural network components
67,RCNNs là gì?,Recurrent Convolutional model để dự đoán sử dụng sequence của images (video). Kết hợp region proposal techniques và CNNs để identify và locate objects trong ảnh.
